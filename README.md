# ğŸ§  Clinical RAG Assistant

A **Retrieval-Augmented Generation (RAG)** chatbot designed for clinical and healthcare use cases. It supports answering user queries based on **internal documents** (PDF, Word, etc.) and **external web search**. Built using LangChain, Streamlit, ChromaDB, and optionally local LLMs via Ollama.

---

## ğŸš€ Features

- ğŸ” Ingest and vectorize clinical documents (PDF, DOCX, TXT, etc.)
- ğŸ’¬ Ask natural language questions and receive contextual answers
- ğŸ“š Displays sources and references alongside each response
- ğŸŒ Optional web search fallback for out-of-domain questions
- ğŸ§  Powered by LangChain with OpenAI or local Ollama models
- ğŸ“¦ Dockerized for easy deployment
- ğŸ§¾ Chat history and document inspection

---

## ğŸ—‚ï¸ Project Structure

â”œâ”€â”€ app.py # Main Streamlit UI

â”œâ”€â”€ ingest.py # Document loader & vector store

â”œâ”€â”€ rag_chain.py # RAG chain definition

â”œâ”€â”€ requirements.txt # Python dependencies

â”œâ”€â”€ Dockerfile # For containerized setup

â”œâ”€â”€ .env # API keys and environment variables

â””â”€â”€ data/ # Directory to place your source documents



---

## âš™ï¸ Local Setup (Recommended for Development)

### 1. Clone the Repository

```bash
git clone https://github.com/Karthik-karnam/RAG_Clinical_chatbot.git
cd clinical-rag-assistant
```


### 2. (Option A) Create a Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. (Option B) Set up with Docker (Recommended for Deployment)

```bash
docker build -t clinical-rag .
docker run -p 8501:8501 -v $(pwd)/data:/app/data clinical-rag
```

# ğŸ¤– Using a Local Model with Ollama (Optional)
If you donâ€™t want to use OpenAI and prefer local models like mistral, install Ollama:

### Step 1: Install Ollama

Follow instructions for your platform: https://ollama.com/download


### Step 2: Pull a model
```bash
ollama pull mistral
```

### Step 3: Run Ollama (auto-starts)
```bash
ollama run mistral
```

Ensure that ```ollama``` is running on ```http://localhost:11434.```

# ğŸ” Environment Variables

Create a ```.env``` file in the project root:


```bash
OPENAI_API_KEY=your_openai_key_here
USE_OLLAMA=true         # set to false if using OpenAI
OLLAMA_MODEL=mistral
```


# ğŸ“ Add Documents for Ingestion

Place your documents (PDF, Word, Excel, TXT) inside the ```/data``` folder. The app will parse and embed these on startup.


# â–¶ï¸ Run the App

If installed locally:


```bash
streamlit run app.py
```

# ğŸ’¬ Demo UI Walkthrough

<img width="1911" height="884" alt="Screenshot 2025-07-21 at 6 02 42â€¯PM" src="https://github.com/user-attachments/assets/55adde74-c3af-4211-a2e1-a95ddbfc4407" />

- Upload documents from the sidebar

- Click â€œProcess Documentsâ€ to embed

- Ask a question in the chat box

- View answers, sources, and metrics

- Export chat history as needed


# ğŸ§ª Test Sample Documents

You can use any dummy clinical documents, or public medical reports (e.g., from WHO, CDC) in ```.pdf``` or ```.docx``` format.


# ğŸ™‹â€â™‚ï¸ Author
Developed by ```Karthik Karnam``` â€” Graduate Researcher at University of Florida
For inquiries or collaboration, please reach out via LinkedIn or email.







